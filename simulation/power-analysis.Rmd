---
title: "Empirical power analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    # css: ../src/writeUp.css
    # includes:
      # after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: inline
---

September 2019

[Script run `r Sys.time()`]

```{r libraries, include=F}

library(tidyverse)  # data wrangling
library(broom)      # vectorised function unpacking
library(ez)         # ANOVA functions with nice mixed syntax

theme_set(theme_light() + theme(
  panel.grid = element_blank()
))

```

# Feelings of Responsibility by Receivership of Reward
## Marwa El Zein and Matt Jaquiery. 

Peopleâ€™s sense of responsibility for decisions is strongly dependent on the success of the outcome. We want to know whether this effect is dependent upon whether the person concerned actually receives the outcome.

## Power analysis using simulations

```{r}
source("generate-data.R")

timeStart <- Sys.time()

effectSizes <- c(0, 1, 3, 5, 10)
ns <- c(5, 10, 20, 35, 50)
reps <- 20

data <- NULL

for (es in effectSizes) {
  for (n in ns) {
    for (r in 1:reps) {
      data <- rbind(data, cbind(
        generateData(n, effectSizeMean = es, effectSizeSD = es / 3),
        tibble(
          effectSize = es, 
          n = n,
          r = r,
          run = paste0('es', es, '_n', n, '_r', r)
        )
      ))
    }
  }
}

timeEnd <- Sys.time()

print(paste0(
  "Data generation complete. Generated ",
  nrow(data), " rows in ",
  round(difftime(timeEnd, timeStart, units = "secs"), 2), "s."
))

```

### Plot data

If the model is generating data correctly we should see some patterns emerging clearly as the effect size increases.

```{r}

data %>% group_by(pid, run, outcome, pGetsOutcome) %>%
  dplyr::filter(n == max(n), effectSize < 15) %>%
  summarise_if(is.numeric, mean) %>%
  gather(key = "decision", value = "scaleResp", scaleResp:scaleRespFinal) %>%
  ggplot(aes(x = decision, y = scaleResp, colour = pGetsOutcome, linetype = outcome)) +
  stat_summary(geom = "line", aes(group = paste(outcome, pGetsOutcome)), 
               fun.y = mean, position = position_dodge(.0)) +
  stat_summary(geom = 'errorbar', aes(group = paste(outcome, pGetsOutcome)), width = 0,
               fun.data = mean_cl_normal, position = position_dodge(.0)) + 
  scale_y_continuous(limits = c(0, 100)) +
  facet_grid(.~effectSize, labeller = label_both) 

```

We told the model that participants should feel more responsible for their own outcomes regardless of valence, and that's what we get back from the model. So far so good. Now we can do statistical tests in preparation for an empirical power analysis. 

### Stats (ANOVA)

```{r}
timeStart <- Sys.time()

suppressWarnings(
  aovs <- data %>%
    gather(key = "decision", value = "scaleResp", scaleResp:scaleRespFinal) %>%
    mutate_at(vars(pid, decision, outcome, pGetsOutcome), factor) %>%
    nest(-run, -effectSize, -r, -n) %>%
    mutate(
      cor = map(data, ~ cor.test(.x$scaleResp, as.numeric(.x$decision))),
      aov = map(data, ~ ezANOVA(data = .x,
                                dv = scaleResp,
                                wid = pid,
                                within = c(
                                  decision,
                                  outcome,
                                  pGetsOutcome
                                  ),
                                return_aov = T))
    )
)

# Extract AoV objects
for (i in 1:nrow(aovs)) {
  aovs$aov[[i]] <- aovs$aov[[i]]$aov
}

aovs <- aovs %>% 
  mutate(tidied = map(aov, tidy)) %>%
  unnest(tidied) %>%
  filter(term != "Residuals") %>%
  select(-term, -df, -sumsq, -meansq, -statistic) %>%
  spread(stratum, p.value)

timeEnd <- Sys.time()

print(paste0(
  "ANOVAs completed. Ran ",
  nrow(aovs), " analyses in ",
  round(difftime(timeEnd, timeStart, units = "secs"), 2), "s."
))

aovs


```

We now have p-values for all those effects for each model. 

### Power analysis

We can now look to see how frequently effects are detected for each of our parameter settings. We have two dimensions, effect size and sample size, so we can make a grid where we colour the cells according to how frequently we get p-values < alpha. Let's start by looking at the main effect of whether the participant gets the outcome.

```{r}

alpha = .05

aovs %>% group_by(effectSize, n) %>%
  summarise(pSig = mean(`pid:pGetsOutcome` < alpha)) %>%
  ggplot(aes(x = n, y = effectSize, fill = pSig)) +
  geom_tile() +
  geom_text(aes(label = pSig), colour = "white") +
  # use the actual values we simulated as labels
  scale_y_continuous(breaks = effectSizes) +
  scale_x_continuous(breaks = ns) +
  labs(
    title = "Heatmap of the probability of a significant result",
    subtitle = paste0("alpha = ", alpha, "; simulations for each cell = ", reps)
    )

```

Because we have no effect of participant getting the outcome where effectSize = 0, the pSig represents a false positive rate. Anywhere else, we know there is an effect, so (1 - pSig) gives the false negative rate. 

### Next steps

We now have the tools to calculate power empirically. We can also adjust the underlying simulation. Currently we model the effect of pGetsOutcome as simply increasing the responsibility ratings - we may wish instead to model it as an interaction. We may also want to explore how the statistics handle the other effects included in the ANOVA model (lots of interactions) - we do not expect with the current model that these ought to be significant, so it'll be useful to see how often they come out as significant in the simulations.
