---
title: "Original study data analysis"
author: "Matt Jaquiery (matt.jaquiery@psy.ox.ac.uk)"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
    # css: ../src/writeUp.css
    # includes:
      # after_body: ../src/toc_menu.html
  html_notebook:
    toc: yes
    toc_depth: 3
editor_options:
  chunk_output_type: inline
---

September 2019

[Script run `r Sys.time()`]

```{r libraries, include=F}

library(tidyverse)  # data wrangling
library(R.matlab)   # import data from MATLAB files

library(ez)         # ANOVA functions with nice mixed syntax

theme_set(theme_light() + theme(
  panel.grid = element_blank()
))

```

# Feelings of Responsibility by Receivership of Reward
## Marwa El Zein and Matt Jaquiery. 

People’s sense of responsibility for decisions is strongly dependent on the success of the outcome

## Magnitude of the outcome

Sofia Bonicalzi 's poster showed that people judged to feel more **in control** when the magnitude of outcome was larger. We can attempt to replicate this with **responsibility rating** and in the context of **group decisions**.

## Group members where the reward is given to only one member (participant/other)

### Is less effort put in/agency felt if they are told beforehand that someone else is getting the reward?

Pat Lockwood’s study on effort expended for self/other rewards

### Are responsibility ratings higher where participant rather than partner gets the reward from a join decision?
     
Paradigm: participants do group decisions (similar to the paradigm of my responsibility study), at the time of the decision they don't know yet who will get the outcome (and they judge how responsible here, which is the baseline of their sense of responsibility while doing a group decision), and at the time where the outcome is shown, they also get the information about who actually got the outcome (participant/other) - and there we can compare the change in their sense of responsibility before and after the outcome, and how it varies as a function of who get the outcome. 

Alternative: show who gets the outcome before the outcome, to see whether even independently of the valence, the sense of responsibility changes. 

[!Paradigm](paradigm.png)

## Simultations using old data

```{r load old data}

data <- NULL

folder <- 'past-data/Online experiment'
for (f in list.files(folder, full.names = T)) {
  if (!grepl('\\.mat$', f)) {
    next()
  }
  contents <- readMat(f)
  tmp <- tibble(
    pid = as.numeric(contents$subid),
    age = as.numeric(contents$agesub),
    gender = as.numeric(contents$gendsub),
    condition = as.numeric(contents$cond),
    group = as.numeric(contents$group),
    img1 = as.numeric(contents$img1),
    img2 = as.numeric(contents$img2),
    choiceButton = as.numeric(contents$buttchoice),
    choiceRT = as.numeric(contents$rtchoice),
    outcome = as.numeric(contents$outcome),
    scaleOrder = as.numeric(contents$scaleorder),
    scaleResp = as.numeric(contents$scaleresp),
    scaleRT = as.numeric(contents$scalert)
  )
  
  data <- rbind(data, tmp)
}

data <- data %>%
  mutate(pid = factor(pid), 
         gender = factor(gender, labels = c('male', 'female')),
         condition = factor(condition, labels = c('alone', 'two',
                                                  'five',
                                                  'two, other plays',
                                                  'two computers',
                                                  'participant + 4 comps',
                                                  'alone, computer plays')),
         img1 = factor(img1),
         img2 = factor(img2),
         outcome = factor(outcome, labels = c('good', 'bad')),
         scaleOrder = factor(scaleOrder)) %>%
  mutate(agency = !(condition %in% c('two, other plays', 'computer plays')))

data
```

### Review old data

If we've got the data set up right, we should see some interesting results by interacting group, outcome, and scale order against the responsibility levels reported by the participants:

```{r old effect visualisation}
dw <- .4

data %>% mutate(alone = group == 0) %>%
  group_by(pid, scaleOrder, outcome, alone) %>%
  summarise(scaleResp = mean(scaleResp)) %>%
  filter_all(all_vars(!is.nan(.))) %>%
  ggplot(aes(x = scaleOrder, y = scaleResp, 
                    colour = outcome, group = outcome)) +
  stat_summary(geom = 'point', fun.y = mean, position = position_dodge(dw)) +
  stat_summary(geom = 'errorbar', fun.data = mean_cl_normal, width = 0, 
               position = position_dodge(dw)) +
  stat_summary(geom = 'line', fun.y = mean, position = position_dodge(dw)) +
  facet_wrap(~alone, labeller = label_both) +
  scale_y_continuous(limits = c(0, 100))

```

We also want to see some general distribution and variability type stuff for participants. The paradigm we'll use is going to be most similar to the condition with one/two players, so we'll take a closer look at the distributions of responsibility questions.

```{r}

data %>% filter(condition == 'five') %>%
  ggplot(aes(x = scaleResp)) +
  geom_density(aes(fill = pid), alpha = .25, colour = NA) +
  geom_density(fill = NA, colour = 'black', linetype = 'dashed') +
  guides(fill = 'none') +
  labs(title = 'Density of responses in paried condition',
       subtitle = 'Dashed outline shows overall average, coloured areas show individual participants')

```

### Create model from old data

We use the summaries for relevant conditions in the original data as a model for simulating new data.

```{r model participants in different conditions}

effectSizeBeneficiary <- 1
effectSizesBeneficiary <- rnorm(length(unique(data$pid)), 
                                mean = effectSizeBeneficiary)
effectSizeInteraction <- 1
effectSizesInteraction <- rnorm(length(unique(data$pid)), 
                                mean = effectSizeInteraction)

model <- data %>% 
  dplyr::filter(condition == 'five', agency == T, scaleOrder == 2) %>%
  group_by(pid, agency, outcome, scaleOrder) %>%
  summarise(choiceRT_m = mean(choiceRT),
            choiceRT_sd = sd(choiceRT),
            scaleResp_m = mean(scaleResp),
            scaleResp_sd = sd(scaleResp),
            scaleRT_m = mean(scaleRT),
            scaleRT_sd = sd(scaleRT)) %>%
  dplyr::filter_all(all_vars(!is.nan(.))) %>% 
  # add effects
  mutate(beneficiaryES = effectSizesBeneficiary[as.numeric(pid)],
         effectSize = effectSizesInteraction[as.numeric(pid)])

# Take out participants who don't give responses for all contingencies.
# 2xOutcome
pids <- model %>% 
  ungroup() %>%
  group_by(pid) %>%
  summarise(n = n()) %>%
  dplyr::filter(n == 2)

model <- model %>% dplyr::filter(pid %in% pids$pid)

model

```

### Simulate new data from model

New data are produced from the model by using the distributions identified from the real participants.

```{r simulate data with new pattern}

simData <- data %>% 
  dplyr::filter(F) %>%            # empty current data
  mutate(pGetsOutcome = F)        # add new columns

# The code is vectorised over reps, so increasing this has less performance
# impact than we might think. This will be useful for bootstrap sampling in
# empirical power analysis later.
reps <- 10

for (pid in unique(model$pid)) {
  for (pGetsOutcome in c(T, F)) {
    for (outcome in c('good', 'bad')) {
      tmp <- dplyr::filter(model, pid == !!pid, outcome == !!outcome)
      
      # Add in the actual effect 
      # (more responsibility if self gets outcome, regardless of valence)
      tmp$scaleResp_m <- tmp$scaleResp_m + 
        (tmp$scaleResp_sd * tmp$beneficiaryES * pGetsOutcome) +
        (tmp$scaleResp_sd * tmp$effectSize * pGetsOutcome * (outcome == 'good'))
      
      simData <- rbind(simData, 
                       tibble(
                         pid = pid,
                         pGetsOutcome,
                         group = 'five',
                         outcome,
                         choiceRT = rnorm(reps, 
                                          tmp$choiceRT_m, 
                                          tmp$choiceRT_sd),
                         scaleResp = rnorm(reps, 
                                           tmp$scaleResp_m, 
                                           tmp$scaleResp_sd),
                         scaleRT = rnorm(reps, 
                                         tmp$scaleRT_m, 
                                         tmp$scaleRT_sd)
                       ))     
    }
  }
}

# Keep the data within scale limits
simData <- simData %>%
  mutate(
    choiceRT = pmax(choiceRT, 0),
    scaleRT = pmax(scaleRT, 0),
    scaleResp = pmin(100, pmax(0, scaleResp))
  )


simData <- simData %>%
  mutate(
    pid = factor(pid),
    group = factor(group),
    outcome = factor(outcome),
    pGetsOutcome = factor(pGetsOutcome)
  )

simData

```

### Plot simulated data

Plot simulated data.

```{r simulated data plots}

simData %>% 
  ggplot(aes(x = pGetsOutcome, y = scaleResp, 
                    colour = outcome, group = outcome)) +
  stat_summary(geom = 'point', fun.y = mean, alpha = .5,
               position = position_dodge(dw)) +
  stat_summary(geom = 'errorbar', fun.data = mean_cl_normal, width = 0, 
               position = position_dodge(dw)) +
  stat_summary(geom = 'line', fun.y = mean, position = position_dodge(dw)) +
  scale_y_continuous(limits = c(0, 100)) +
  labs(x = "Participant is beneficiary", 
       y = "Retrospective feeling of responsibility for decision")

```

### Analysis of simulated data

Test simulated data.

```{r simultaed data ANOVA}

result <- simData %>% 
  ezANOVA(
  dv = scaleResp,
  wid = pid,
  within = c(
    outcome,
    pGetsOutcome
  )
)

result

```

So here we see we are unable to detect the effect we placed within the data for the interaction. The power analysis script does more digging in this regard, allowing us to estimate more precisely how large effects need to be before they are reliably detected.
